# VLLM Worker for RunPod

This repository provides a minimal serverless-compatible VLLM worker setup for RunPod, with support for dynamic Hugging Face model downloading via environment variables.

> ‚ö†Ô∏è **Note:** This is essentially a wrapper around RunPod‚Äôs base VLLM Docker image with the added ability to specify a custom model through the `MODEL_NAME` environment variable. The `download_model.py` logic is derived from RunPod‚Äôs own model downloader and may be deprecated if their base repo includes the same functionality in the future.

---

## üöÄ Features

* ‚úÖ Dynamic model downloading using `huggingface_hub`
* ‚úÖ Supports `.safetensors`, `.bin`, `.pt` model formats
* ‚úÖ Auto-downloads both model and tokenizer (if separate)
* ‚úÖ Optional HF token authentication via `HF_TOKEN`
* ‚úÖ Generates `local_model_args.json` for downstream usage

---

## üß± Environment Variables

| Variable             | Description                                                             | Required |
| -------------------- | ----------------------------------------------------------------------- | -------- |
| `MODEL_NAME`         | Hugging Face model repo name (e.g. `TheBloke/Mistral-7B-Instruct-v0.1`) | ‚úÖ        |
| `MODEL_REVISION`     | Optional Git revision (`main`, commit SHA, etc.)                        | ‚ùå        |
| `TOKENIZER_NAME`     | Optional separate tokenizer repo name                                   | ‚ùå        |
| `TOKENIZER_REVISION` | Optional tokenizer revision                                             | ‚ùå        |
| `HF_HOME`            | Cache/download directory for models and tokenizers                      | ‚úÖ        |
| `HF_TOKEN`           | Hugging Face token (for gated/private models)                           | ‚ùå        |

---

## üê≥ Docker Usage

This Dockerfile:

* Installs dependencies
* Runs `download_model.py`
* Starts the VLLM OpenAI-compatible server

**Build image:**

```bash
docker build -t vllm-runpod .
```

**Run container:**

```bash
docker run --rm -e MODEL_NAME=TheBloke/Mistral-7B-Instruct-v0.1 -e HF_HOME=/models vllm-runpod
```

---

## üìÇ Output

On successful startup, the following are generated inside your container:

* Downloaded model and tokenizer in `$HF_HOME`
* `local_model_args.json` with resolved model/tokenizer paths

---

## üìù Example `CMD` in Dockerfile

```dockerfile
CMD ["bash", "-c", "python /workspace/download_model.py && python3 -m vllm.entrypoints.openai.api_server --model /"]
```

---

## üîß Notes

* Only `MODEL_NAME` is required. If tokenizer files are part of the model repo, separate `TOKENIZER_NAME` is not needed.
* Tensorization is stubbed but not enabled yet ‚Äî you can enable it once VLLM supports tensorized model loading properly.

---

## üì¶ Credits

* Based on [RunPod‚Äôs official vllm-worker](https://github.com/runpod-workers/vllm-worker).
* Hugging Face model handling via [`huggingface_hub`](https://github.com/huggingface/huggingface_hub).
